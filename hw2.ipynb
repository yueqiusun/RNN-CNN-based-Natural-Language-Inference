{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence1\tsentence2\tlabel\r\n",
      "A young girl in a pink shirt sitting on a dock viewing a body of water .\tA young girl watching the sunset over the water .\tneutral\r\n",
      "A woman is smiling while the man next to her is focused on a blue object with a pattern on it .\tTwo people are next to each other .\tentailment\r\n",
      "Across the river , you can see a large building .\tThe large building is full of apartments and tenants\tneutral\r\n",
      "a man in white shorts and a black shirt is paragliding on the ocean\tA man is riding a jetski on the ocean .\tcontradiction\r\n",
      "Four black dogs run together on bright green grass .\tFour dogs are preparing to be launched into space .\tcontradiction\r\n",
      "A female laying on her stomach in the water outside with umbrellas .\tThere is a women outdoors\tentailment\r\n",
      "Children eat at a long table with black chairs .\tKids at a short table with red chairs .\tcontradiction\r\n",
      "A person rides a motorcycle quickly .\tThe man is racing his motorcycle in a race .\tneutral\r\n",
      "Woman riding a red bicycle down a city street , with a few people in the background .\tperson riding a bike\tentailment\r\n"
     ]
    }
   ],
   "source": [
    "!head ./hw2_data/snli_train.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets improve libraries that we are going to be used in this lab session\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "random.seed(134)\n",
    "\n",
    "from bagofwords import BagOfWords\n",
    "from module import Module as M\n",
    "from hyperparameter import Hyperparameter as hp\n",
    "import RR\n",
    "\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize   \n",
    "from module import Module as M\n",
    "from train_f import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_all_tokens(inp1, inp2):\n",
    "    all_tokens = []\n",
    "    for sent in inp1:\n",
    "        for x in sent:\n",
    "            all_tokens.append(x)   \n",
    "    for sent in inp2:\n",
    "        for x in sent:\n",
    "            all_tokens.append(x)    \n",
    "    \n",
    "    return all_tokens\n",
    "    \n",
    "def read_data(filename, count = 9999999):\n",
    "    x1 = []\n",
    "    x2 = []\n",
    "    y = []\n",
    "    x1_w = []\n",
    "    x2_w = []\n",
    "    raw_data = []\n",
    "    filename =hp.prepath_data + filename\n",
    "    len_list = []\n",
    "    with open(filename, \"r\") as f:\n",
    "\n",
    "        line_num = 0\n",
    "        line = f.readline()\n",
    "        line = f.readline()\n",
    "        while line != None and line != \"\" and line_num<count:\n",
    "            line = line.lower()\n",
    "            arr = line.split('\\t')\n",
    "            #list of length of  sentences\n",
    "            x1.append(arr[0])\n",
    "            x2.append(arr[1])\n",
    "            y.append(hp.dummy2int[arr[2][:-1]])\n",
    "            line_num += 1\n",
    "            raw_data.append(line)\n",
    "            line = f.readline()\n",
    "        x1_w = [line.split() for line in x1]\n",
    "        x2_w = [line.split() for line in x2]\n",
    "        len_list = [len(line.split()) for line in x1]\n",
    "        len_list += [len(line.split()) for line in x2]\n",
    "        # take the 99 precentile of the length, let it be maximum sentence length\n",
    "        if count < 1000:\n",
    "            MAX_SENTENCE_LENGTH = sorted(len_list, reverse = True)[0]\n",
    "        else:\n",
    "            MAX_SENTENCE_LENGTH = sorted(len_list, reverse = True)[1000]\n",
    "        a = sorted(len_list, reverse = True)\n",
    "        \n",
    "        all_tokens = build_all_tokens(x1_w, x2_w)\n",
    "    return x1_w, x2_w, y, MAX_SENTENCE_LENGTH, all_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 9999999999\n",
    "train_x1, train_x2, train_y, MAX_SENTENCE_LENGTH, all_train_tokens  = read_data('snli_train.tsv', count = count)\n",
    "val_x1, val_x2, val_y, _, _ = read_data('snli_val.tsv', count = count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "max_vocab_size = 20000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset 1 size is 100000\n",
      "Train dataset 2 size is 100000\n",
      "Val dataset 1 size is 1000\n",
      "Val dataset 2 size is 1000\n"
     ]
    }
   ],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_x1_indices = token2index_dataset(train_x1)\n",
    "train_x2_indices = token2index_dataset(train_x2)\n",
    "val_x1_indices = token2index_dataset(val_x1)\n",
    "val_x2_indices = token2index_dataset(val_x2)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset 1 size is {}\".format(len(train_x1_indices)))\n",
    "print (\"Train dataset 2 size is {}\".format(len(train_x2_indices)))\n",
    "print (\"Val dataset 1 size is {}\".format(len(val_x1_indices)))\n",
    "print (\"Val dataset 2 size is {}\".format(len(val_x2_indices)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ft_home = './'\n",
    "words_to_load = 50000\n",
    "emb_dim =  300\n",
    "\n",
    "with open(ft_home + 'wiki-news-300d-1M.vec') as f:\n",
    "    loaded_embeddings_ft = np.zeros((len(id2token), emb_dim))\n",
    "    words_ft = {}\n",
    "    idx2words_ft = {}\n",
    "    ordered_words_ft = []\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= words_to_load: \n",
    "            break\n",
    "        s = line.split()\n",
    "        #if the word in vocabulary is in fasttext, we load the embedding for that word.\n",
    "        if s[0] in token2id:\n",
    "            idx = token2id[s[0]]\n",
    "            loaded_embeddings_ft[idx] = np.asarray(s[1:])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#if the word in vocabulary is not in fasttext(include unk and pad), we initialize a random vector.\n",
    "for i in range(len(id2token)):\n",
    "    if loaded_embeddings_ft[i][0] == 0:\n",
    "        loaded_embeddings_ft[i] = np.random.normal(scale=0.6, size=(emb_dim, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label2onehot_helper(x):\n",
    "    if x == 0:\n",
    "        return np.array([1,0,0])\n",
    "    elif x == 1:\n",
    "        return np.array([0,1,0])\n",
    "    elif x == 2:\n",
    "        return np.array([0,0,1])\n",
    "def label2onehot(labels):\n",
    "    output = np.zeros((len(labels), 3))\n",
    "    for i, x in enumerate(labels):\n",
    "        output[i] = label2onehot_helper(x)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class SNLIDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list1, data_list2,target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of SNLI tokens \n",
    "        @param target_list: list of SNLI targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list1 = data_list1\n",
    "        self.data_list2 = data_list2\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list1) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list1)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx1 = self.data_list1[key][:MAX_SENTENCE_LENGTH]\n",
    "        token_idx2 = self.data_list2[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx1, token_idx2, len(token_idx1), len(token_idx2),label]\n",
    "\n",
    "def SNLI_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list1 = []\n",
    "    data_list2 = []\n",
    "    label_list = []\n",
    "    length_list1 = []\n",
    "    length_list2 = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[4])\n",
    "        length_list1.append(datum[2])\n",
    "        length_list2.append(datum[3])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec1 = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[2])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        padded_vec2 = np.pad(np.array(datum[1]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[3])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list1.append(padded_vec1)\n",
    "        data_list2.append(padded_vec2)\n",
    "    \n",
    "    #transform labels to one hot label to fit into cross-entropy loss\n",
    "    label_list_onehot = label2onehot(label_list)\n",
    "    # define example\n",
    "    \n",
    "    \n",
    "    \n",
    "    return [torch.from_numpy(np.array(data_list1)), torch.from_numpy(np.array(data_list2)),\n",
    "            torch.LongTensor(length_list1), torch.LongTensor(length_list2),torch.LongTensor(label_list_onehot)]\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = SNLIDataset(train_x1_indices, train_x1_indices,train_y)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=SNLI_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = SNLIDataset(val_x1_indices, val_x1_indices, val_y)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=SNLI_collate_func,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_emb_layer(weights_matrix, non_trainable=False):\n",
    "    weights_matrix = torch.from_numpy(weights_matrix)\n",
    "    num_embeddings, embedding_dim = weights_matrix.size()\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "    emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "\n",
    "    return emb_layer, num_embeddings, embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, num_classes, vocab_size):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        #self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=PAD_IDX)\n",
    "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix, False)\n",
    "\n",
    "        self.conv1 = nn.Conv1d(emb_size, hidden_size, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding=1)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        batch_size, seq_len = x.size()\n",
    "\n",
    "        embed = self.embedding(x)\n",
    "        hidden = self.conv1(embed.transpose(1,2)).transpose(1,2)\n",
    "        hidden = F.relu(hidden.contiguous().view(-1, hidden.size(-1))).view(batch_size, seq_len, hidden.size(-1))\n",
    "\n",
    "        hidden = self.conv2(hidden.transpose(1,2)).transpose(1,2)\n",
    "        hidden = F.relu(hidden.contiguous().view(-1, hidden.size(-1))).view(batch_size, seq_len, hidden.size(-1))\n",
    "\n",
    "        hidden = torch.sum(hidden, dim=1)\n",
    "        logits = self.linear(hidden)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, weights_matrix, hidden_size, num_layers, num_classes):\n",
    "        # RNN Accepts the following hyperparams:\n",
    "        # emb_size: Embedding Size\n",
    "        # hidden_size: Hidden Size of layer in RNN\n",
    "        # num_layers: number of layers in RNN\n",
    "        # num_classes: number of output classes\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        #self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=PAD_IDX)\n",
    "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix, False)\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_size, num_layers, batch_first=True, bidirectional = True) #dim1: batch dim2: sequence dim3: emb\n",
    "        self.linear1 = nn.Linear(hidden_size * 2, hidden_size * 2)\n",
    "        self.linear2 = nn.Linear(hidden_size * 2, num_classes)\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Function initializes the activation of recurrent neural net at timestep 0\n",
    "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
    "        hidden = torch.randn(self.num_layers * 2, batch_size, self.hidden_size)\n",
    "\n",
    "        return hidden\n",
    "        \n",
    "    def forward(self, x1,x2):\n",
    "        # reset hidden state\n",
    "\n",
    "        batch_size, seq_len = x1.size()\n",
    "        \n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "\n",
    "        # get embedding of characters\n",
    "        #embed = self.embedding(x)\n",
    "        # pack padded sequence\n",
    "        #embed = torch.nn.utils.rnn.pack_padded_sequence(embed, lengths.numpy(), batch_first=True)\n",
    "        # fprop though RNN\n",
    "        embed1 = self.embedding(x1)\n",
    "        embed2 = self.embedding(x2)\n",
    "        rnn_out1, self.hidden1 = self.rnn(embed1, self.hidden)\n",
    "        rnn_out2, self.hidden2 = self.rnn(embed2, self.hidden)\n",
    "        rnn_out = torch.cat((rnn_out1, rnn_out2),1)\n",
    "        linear_out1 = self.linear1(rnn_out)\n",
    "        logits = self.linear2(linear_out1)\n",
    "        # undo packing\n",
    "        #rnn_out, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out, batch_first=True)\n",
    "        # sum hidden activations of RNN across time\n",
    "        #rnn_out = torch.sum(rnn_out, dim=1)\n",
    "        \n",
    "        #logits = self.linear(rnn_out)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-e18501d2b34a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Backward and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# validate every 100 iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data1,data2,lengths1,length2,labels in loader:\n",
    "        data1_batch, data2_batch,lengths1_batch,lengths2_batch, label_batch = data1,data2,lengths1,length2,labels\n",
    "        outputs = F.softmax(model(data1_batch, data2_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "\n",
    "model = RNN(weights_matrix=loaded_embeddings_ft, hidden_size=200, num_layers=1, num_classes=3)\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 2 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    losses = AverageMeter()\n",
    "    for i, (x1, x2, lengths1, lengths2, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(x1, x2)\n",
    "        loss = criterion(outputs, labels)\n",
    "        losses.update(loss, x1.size(0))\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            print(' Epoch: [{}/{}], Step: [{}/{}], Training loss: {loss.avg:.4f}, Validation Acc: {}'.format(\n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), loss = losses, val_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = AverageMeter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(data):\n",
    "    # Returns:\n",
    "    # id2char: list of chars, where id2char[i] returns char that corresponds to char i\n",
    "    # char2id: dictionary where keys represent chars and corresponding values represent indices\n",
    "    # some preprocessing\n",
    "    max_len = max([len(word[0]) for word in data])\n",
    "    all_chars = []\n",
    "    for word in data:\n",
    "        all_chars += word[0]\n",
    "    unique_chars = list(set(all_chars))\n",
    "\n",
    "    id2char = unique_chars\n",
    "    char2id = dict(zip(unique_chars, range(2,2+len(unique_chars))))\n",
    "    id2char = ['<pad>', '<unk>'] + id2char\n",
    "    char2id['<pad>'] = PAD_IDX\n",
    "    char2id['<unk>'] = UNK_IDX\n",
    "\n",
    "    return char2id, id2char, max_len\n",
    "\n",
    "def convert_to_chars(data):\n",
    "    return [([c for c in sample[0]], sample[1]) for sample in data]\n",
    "\n",
    "### Function that preprocessed dataset\n",
    "def read_data():\n",
    "    data = pkl.load(open(\"data.p\", \"rb\"))\n",
    "    train_data, val_data, test_data = data['train'], data['valid'], data['test']\n",
    "    train_data, val_data, test_data = convert_to_chars(train_data), convert_to_chars(val_data), convert_to_chars(test_data)\n",
    "    char2id, id2char, max_len = build_vocab(train_data)\n",
    "    return train_data, val_data, test_data, char2id, id2char, max_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data, char2id, id2char, MAX_WORD_LENGTH = read_data()\n",
    "\n",
    "print (\"Maximum word length of dataset is {}\".format(MAX_WORD_LENGTH))\n",
    "print (\"Number of characters in dataset is {}\".format(len(id2char)))\n",
    "print (\"Characters:\")\n",
    "print (char2id.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pkl.load(open(\"data.p\", \"rb\"))\n",
    "train_data, val_data, test_data = data['train'], data['valid'], data['test']\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets build the PyTorch DataLoader as we did in previous lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VocabDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_tuple, char2id):\n",
    "        \"\"\"\n",
    "        @param data_list: list of character\n",
    "        @param target_list: list of targets\n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list, self.target_list = zip(*data_tuple)\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "        self.char2id = char2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        char_idx = [self.char2id[c] if c in self.char2id.keys() else UNK_IDX  for c in self.data_list[key][:MAX_WORD_LENGTH]]\n",
    "        label = self.target_list[key]\n",
    "        return [char_idx, len(char_idx), label]\n",
    "\n",
    "def vocab_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]),\n",
    "                                pad_width=((0,MAX_WORD_LENGTH-datum[1])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    ind_dec_order = np.argsort(length_list)[::-1]\n",
    "    data_list = np.array(data_list)[ind_dec_order]\n",
    "    length_list = np.array(length_list)[ind_dec_order]\n",
    "    label_list = np.array(label_list)[ind_dec_order]\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build train, valid and test dataloaders\n",
    "\n",
    "train_dataset = VocabDataset(train_data, char2id)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = VocabDataset(val_data, char2id)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = VocabDataset(test_data, char2id)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets implement basic Recurrent Neural Net model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important things to keep in mind when using variable sized sequences in RNN in Pytorch\n",
    "\n",
    "### RNN modules accept packed sequences as inputs\n",
    "* pack_padded_sequence function packs a sequence (in Tensor format) containing padded sequences of variable length. **IMPORTANT: the sequences should be sorted by length in a decreasing order before passing to this function**\n",
    "\n",
    "* pad_packed_sequence function is an inverse operation to pack_padded_sequence. Transforms a padded sequence into a tensor of variable lenth sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1:\n",
    "### Implement LSTM cell instead of RNN cell. Train the model and compare the results.\n",
    "### Hint (modify init_hidden function and cell in __init__) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2:\n",
    "### Implement Bidirectional LSTM. You can do it very easily by adding one argument to cell when you create it.\n",
    "### For better understanding we recommend that you implement it youself by reversing a sequence and passing it to another cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3:\n",
    "\n",
    "### Add max-pooling (over time) after passing through RNN instead of summing over hidden layers through time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets implement basic Convolutional Neural Net model for text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important things to keep in mind when using Convolutional Nets for Language Tasks in Pytorch\n",
    "\n",
    "### Conv1d module expect input of size (batch_size, num_channels, length), where in our case input has size (batch_size, length, num_channels). Hence it is important call transpose(1,2) before passing it to convolutional layer and then reshape it back to (batch_size, length, num_channels) by calling transpose(1,2) again\n",
    "\n",
    "### Additionally we need to reshape hidden activations into 2D tensor before passing it to Relu layer by calling view(-1, hidden.size(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, lengths_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, lengths_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "\n",
    "model = CNN(emb_size=100, hidden_size=200, num_layers=2, num_classes=5, vocab_size=len(id2char))\n",
    "\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(data, lengths)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4:\n",
    "### Implement Gated Relu activations as well as Gated Linear activations and compare them with Relu (reference: https://arxiv.org/pdf/1612.08083.pdf )\n",
    "### Hint: Gated Relu activations are sigmoid(conv1_1(x)) * relu(conv1_2(x))\n",
    "### Hint: Gated Linear activations are sigmoid(conv1_1(x)) * conv1_2(x)\n",
    "\n",
    "### Feel free to play with other variants of gating\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5:\n",
    "\n",
    "### Add max-pooling (over time) after passing through conv as well as add non-linear fully connected layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6:\n",
    "\n",
    "### Use Bag-of-Words and Bag-of-NGrams model for this task and compare it with RNN and CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7:\n",
    "\n",
    "### Use FastText for this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
